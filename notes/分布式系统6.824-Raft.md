# 分布式系统6.824-Raft(四)
> 即阅读和学习 `In search of an Understandable Consensus Algorithm` __Raft paper__

## 概述

`Raft`是管理复制日志的一种一致性. 它提供了与`Paxos`一样的算法功能, 但是它的算法结构比`Paxos`更加易于理解, 更容易构建实际的系统. `Raft`将一致性算法分成几个关键性的模块, 例如领导选举, 日志复制, 安全性. `Raft`通过更强的一致性来减少需要考虑的状态.
`Raft`提供了一种新的机制用来改变集群的状态, 并使用重叠大多数来保证安全性.

## 简介

`Raft` 一致性算法就是这些工作的结果。在设计`Raft`算法的时候，我们使用一些特别的技巧来提升它的可理解性，包括算法分解（`Raft`主要被分成了领导人选举，日志复制和安全三个模块）和减少状态机的状态（相对于 Paxos，Raft 减少了非确定性和服务器互相处于非一致性的方式）。

Raft 算法在许多方面和现有的一致性算法都很相似（主要是 Oki 和 Liskov 的 Viewstamped Replication），但是它也有一些独特的特性：

+ 强领导者：和其他一致性算法相比，Raft 使用一种更强的领导能力形式。比如，日志条目只从领导者发送给其他的服务器。这种方式简化了对复制日志的管理并且使得 Raft 算法更加易于理解。
+ 领导选举：Raft 算法使用一个随机计时器来选举领导者。这种方式只是在任何一致性算法都必须实现的心跳机制上增加了一点机制。在解决冲突的时候会更加简单快捷。
+ 成员关系调整：Raft 使用一种共同一致的方法来处理集群成员变换的问题，在这种方法下，处于调整过程中的两种不同的配置集群中大多数机器会有重叠，这就使得集群在成员变换的时候依然可以继续工作。



## Raft 一致性算法

### Raft 特性
+ __Election Safety__ ：在一个Term中最多选举出一个Leader。
+ __Leader Append-Only__ ：Leader不会重写或删除Log entries，只会追加Log 。entries（Election restriction：保证每次选举出来的Leader保存了所有已commit的Log entries）
+ __Log Matching__：如果两个节点上的某个Log entry的Index和Term一致，Index在该Log entry之前的Log entries完全一致。（Committing entries from previous terms）
+ __Leader Completeness__：如果某个Log entry在某个Term被commit，那么之后的Term的Leader都会含有这个Log entry。
+ __State Machine Safety__：如果一个节点将某个Log entry应用到状态机中，其它节点中应用相同Index到状态机的Log entry相同。

### Raft 中的一些关键的信息

#### 1. 状态

|状态|所有服务器上持久存在的|
|-------|------|
|currentTerm | 服务器最后一次知道的任期号（初始化为 0，持续递增）|
|votedFor | 在当前获得选票的候选人的 Id|
| log[] | 日志条目集；每一个条目包含一个用户状态机执行的指令，和收到时的任期号 |

|状态|所有服务器上经常变的|
|-------|------|
| commitIndex| 已知的最大的已经被提交的日志条目的索引值|
| lastApplied| 最后被应用到状态机的日志条目索引值（初始化为 0，持续递增）|

| 状态 | 在领导人里经常改变的 （选举后重新初始化）|
|----|--------|
| nextIndex[] | 对于每一个服务器，需要发送给他的下一个日志条目的索引值（初始化为领导人最后索引值加一）|
| matchIndex[] | 对于每一个服务器，已经复制给他的日志的最高索引值|

#### 2. RPC
由领导人负责调用来复制日志指令；也会用作heartbeat

| 参数 | 解释 |
|----|----|
|term| 领导人的任期号|
|leaderId| 领导人的 Id，以便于跟随者重定向请求|
|prevLogIndex|新的日志条目紧随之前的索引值|
|prevLogTerm|prevLogIndex 条目的任期号|
|entries[]|准备存储的日志条目（表示心跳时为空；一次性发送多个是为了提高效率）
|leaderCommit|领导人已经提交的日志的索引值|

| 返回值| 解释|
|---|---|
|term|当前的任期号，用于领导人去更新自己|
|success|跟随者包含了匹配上 prevLogIndex 和 prevLogTerm 的日志时为真|

接收者实现：

1. 如果 `term < currentTerm` 就返回 false （5.1 节）
2. 如果日志在 prevLogIndex 位置处的日志条目的任期号和 prevLogTerm 不匹配，则返回 false （5.3 节）
3. 如果已经存在的日志条目和新的产生冲突（索引值相同但是任期号不同），删除这一条和之后所有的 （5.3 节）
4. 附加任何在已有的日志中不存在的条目
5. 如果 `leaderCommit > commitIndex`，令 commitIndex 等于 leaderCommit 和 新日志条目索引值中较小的一个

**请求投票 RPC**：

由候选人负责调用用来征集选票

| 参数 | 解释|
|---|---|
|term| 候选人的任期号|
|candidateId| 请求选票的候选人的 Id |
|lastLogIndex| 候选人的最后日志条目的索引值|
|lastLogTerm| 候选人最后日志条目的任期号|

| 返回值| 解释|
|---|---|
|term| 当前任期号，以便于候选人去更新自己的任期号|
|voteGranted| 候选人赢得了此张选票时为真|

接收者实现：

1. 如果`term < currentTerm`返回 false （5.2 节）
2. 如果 votedFor 为空或者就是 candidateId，并且候选人的日志至少和自己一样新，那么就投票给他（5.2 节，5.4 节）

**所有服务器需遵守的规则**：

所有服务器：

* 如果`commitIndex > lastApplied`，那么就 lastApplied 加一，并把`log[lastApplied]`应用到状态机中（5.3 节）
* 如果接收到的 RPC 请求或响应中，任期号`T > currentTerm`，那么就令 currentTerm 等于 T，并切换状态为跟随者（5.1 节）

跟随者（5.2 节）：

* 响应来自候选人和领导者的请求
* 如果在超过选举超时时间的情况之前都没有收到领导人的心跳，或者是候选人请求投票的，就自己变成候选人

候选人（5.2 节）：

* 在转变成候选人后就立即开始选举过程
	* 自增当前的任期号（currentTerm）
	* 给自己投票
	* 重置选举超时计时器
	* 发送请求投票的 RPC 给其他所有服务器
* 如果接收到大多数服务器的选票，那么就变成领导人
* 如果接收到来自新的领导人的附加日志 RPC，转变成跟随者
* 如果选举过程超时，再次发起一轮选举

领导人：

* 一旦成为领导人：发送空的附加日志 RPC（心跳）给其他所有的服务器；在一定的空余时间之后不停的重复发送，以阻止跟随者超时（5.2 节）
*  如果接收到来自客户端的请求：附加条目到本地日志中，在条目被应用到状态机后响应客户端（5.3 节）
*  如果对于一个跟随者，最后日志条目的索引值大于等于 nextIndex，那么：发送从 nextIndex 开始的所有日志条目：
	* 如果成功：更新相应跟随者的 nextIndex 和 matchIndex
	* 如果因为日志不一致而失败，减少 nextIndex 重试
* 如果存在一个满足`N > commitIndex`的 N，并且大多数的`matchIndex[i] ≥ N`成立，并且`log[N].term == currentTerm`成立，那么令 commitIndex 等于这个 N （5.3 和 5.4 节）

### Raft 基础
一个 Raft 集群包含若干个服务器节点；通常是 5 个，这允许整个系统容忍 2 个节点的失效。在任何时刻，每一个服务器节点都处于这三个状态之一：领导人、跟随者或者候选人。在通常情况下，系统中只有一个领导人并且其他的节点全部都是跟随者。跟随者都是被动的：他们不会发送任何请求，只是简单的响应来自领导者或者候选人的请求。领导人处理所有的客户端请求（如果一个客户端和跟随者联系，那么跟随者会把请求重定向给领导人）。


### Raft 领导人选举
Raft 使用一种心跳机制来触发领导人选举。当服务器程序启动时，他们都是跟随者身份。一个服务器节点继续保持着跟随者状态直到他从领导人或者候选者处接收到有效的 RPCs。领导者周期性的向所有跟随者发送心跳包（即不包含日志项内容的附加日志项 RPCs）来维持自己的权威。如果一个跟随者在一段时间里没有接收到任何消息，也就是**选举超时**，那么他就会认为系统中没有可用的领导者,并且发起选举以选出新的领导者。

### Raft 日志复制
一旦一个领导人被选举出来，他就开始为客户端提供服务。客户端的每一个请求都包含一条被复制状态机执行的指令。领导人把这条指令作为一条新的日志条目附加到日志中去，然后并行的发起附加条目 RPCs 给其他的服务器，让他们复制这条日志条目。当这条日志条目被安全的复制（下面会介绍），领导人会应用这条日志条目到它的状态机中然后把执行的结果返回给客户端。如果跟随者崩溃或者运行缓慢，再或者网络丢包，领导人会不断的重复尝试附加日志条目 RPCs （尽管已经回复了客户端）直到所有的跟随者都最终存储了所有的日志条目。

## Raft 算法流程

+ 1. 在一个Raft Group中，初始的状态是所有节点都是Follower。
+ 2. 在经过随机的超时时长后某一个Follower开始Leader election并成为Leader. 其中有两个重要的阶段:
	-  __election timeout__ 中第一个达到超时时间之后, 成为候选者,发送`RequestVote` , 得到大多数表, 最终成为leader. `leader election`由`heartbeat`机制触发
	-  __heartbeat timeout__ leader在以该时间间隔发送`AppendEntries`消息,其中有一些`AppendEntries`不包含任何的log信息, `LogReplication`的信息通过`AppendEntroies`来通过客户端传递到整个server. 确保整个分布式系统的可用性和一致性, 当leader奔溃之后, 其他follower将不会接收到信息, 再过一个`election timeout`之后进行新一轮的选举.
+ 3. 客户端在请求Raft Group时只会将命令发送给Leader，Leader通过Log replication将Log entries复制到其他Follower。在收到超过半数的Follower复制成功的消息后commit该条Log entry并放到应用到状态机中.
+ 4. 状态机执行命令成功后将结果返回客户端.
+ 5. 任何节点在log[]队列增长到一定长度后将执行Snapshot操作。
+ 6. 若Leader故障，最先超时的Follower变为Candidate发起Leader election。
+ 7. 若Candidate成功成为Leader，执行2-4。若Split votes，重新开始Leader election。无论如何currentTerm加1.
+ 8. 若超过半数节点故障，整个Raft Group将故障。（没有节点能成为Leader）
+ 9.节点故障后恢复，将接收当前Leader的心跳，发现currentTerm小于新Leader的currentTerm，自己变为Follower.
+ 10.网络分区故障后恢复，所有Follower都会回滚未commit的`Log entries`并从当前Leader处复制. 保证大多数的正确性和一致性.

## FAQ

#### Raft如何避免分票?

每一个`server`选取一个随机的`election timeout`, 随机性保证了最小的`election 
timeout`的`server`先发出请求投票.  其他的`server`将会很快的看到`new leader`的`HeartBeat`信息, 从而不会变成`candiodates`. 

当极端情况下, 出现了两个`server`同时发生分票的现象时, 则再进行一轮新的投票.

#### 如果旧的`leader`没有发现新的`leader`已被选出怎么办?

通常在如下两种情况下出现上述情况:

+ `old leader`没有收到之前选举投票的信息
+ `old leader`在一个小部分被分割的网络中

`new leader`意味着超过半数的`server`的`currentTerm`为原来`+1`. 则`old leader`将不会提交任何`AppendEntries`操作, 因此, 也不会出现`split brain`的现象.

但是那小部分被分割的网络可能会接收到`old leader`的`AppendEntries`操作, 则这就使得小部分网络的日志信息和大多数日志信息出现分歧.

## Reference
[MIT 6.824 Raft Lecture](https://pdos.csail.mit.edu/6.824/notes/l-raft.txt)

[Raft paper](https://pdos.csail.mit.edu/6.824/papers/raft-extended.pdf)

[Raft 在线演示](http://thesecretlivesofdata.com/raft/)